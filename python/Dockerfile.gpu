# MT5 HTTP inference server (GPU-ready Docker)
# - Uses NVIDIA NGC PyTorch image (includes CUDA + PyTorch)
# - Runs the same MT5 HTTP server as the standard image
#
# NOTE:
# - This does not magically speed up rule-based logic.
# - GPU becomes useful only if you enable GPU-heavy modules (e.g. Antigravity/ML) in the future.

FROM nvcr.io/nvidia/pytorch:25.09-py3

WORKDIR /app

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    TZ=Asia/Tokyo

# System deps (kept minimal)
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
  && rm -rf /var/lib/apt/lists/*

# Install Python deps first (better layer caching)
COPY python/requirements.docker.txt /app/requirements.docker.txt
RUN pip install --no-cache-dir -r /app/requirements.docker.txt

# Copy server + modules
COPY python/ /app/

EXPOSE 5001

CMD ["python", "inference_server_http_7module.py"]
