# GPU専用（CPUは使わない運用）
# 正本: python/inference_server_http_7module.py
# - GET  /health
# - POST /analyze  (MT5 EA: OHLCV配列)
# - POST /predict  (フラット形式)
services:
  mt5-inference:
    build:
      context: .
      dockerfile: python/Dockerfile.gpu
    image: mt5-oanda-trader-inference
    container_name: mt5-inference-server
    ports:
      - "5001:5001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    restart: unless-stopped
    environment:
      - PYTHONUNBUFFERED=1
      - TZ=Asia/Tokyo
      # 安全運用（落ちない/待たない）
      - REQUEST_TIMEOUT_SEC=3.0
      - MAX_WORKERS=4
      # 任意: 既定プリセットやLM Studio URLをここで固定できる
      # - PRESET=antigravity_pullback
      # - STRATEGY=full
      # - LM_STUDIO_URL=http://host.docker.internal:1234
